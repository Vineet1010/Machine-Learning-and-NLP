{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "#df = pd.read_csv(\"https://media-doselect.s3.amazonaws.com/generic/31v2GgzkXJ77Lekd3NeWpd9Ke/training-Reuters.zip\")\n",
    "df = pd.read_csv(\"Reuters.csv\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "tokens = df['article_text'].apply(word_tokenize)\n",
    "count = len(tokens)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "reuters_stopwords = set(stopwords.words('english') + list (punctuation))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = 'word', min_df = 20, max_df = 1000, lowercase = True, stop_words = reuters_stopwords)\n",
    "\n",
    "unique_words = vectorizer.fit_transform(df['article_text'])\n",
    "rows, columns = unique_words.shape\n",
    "\n",
    "with open(\"output.csv\", mode = \"w\") as f:\n",
    "    f.write(str(count))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5, 6, 7)\n"
     ]
    }
   ],
   "source": [
    "list_1 = []\n",
    "list_1.append([(1, 2, 3), {1: (4, 5, 6, 7)}, [8, 9]])\n",
    "print(list_1[0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "#df = pd.read_csv(\"https://media-doselect.s3.amazonaws.com/generic/31v2GgzkXJ77Lekd3NeWpd9Ke/training-Reuters.zip\")\n",
    "df = pd.read_csv(\"Reuters.csv\")\n",
    "\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "artlist = df['article_text'].tolist()\n",
    "tokens = []\n",
    "for i in artlist:\n",
    "    tokens.append(word_tokenize(i))\n",
    "ordered_tokens = []\n",
    "for i in tokens:\n",
    "    for j in i:\n",
    "        ordered_tokens.append(j)\n",
    "count = pd.Series(ordered_tokens).nunique()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "reuters_stopwords = set(stopwords.words('english') + list (punctuation))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = 'word', min_df = 20, max_df = 1000, lowercase = True, stop_words = reuters_stopwords)\n",
    "\n",
    "unique_words = vectorizer.fit_transform(df['article_text'])\n",
    "rows, columns = unique_words.shape\n",
    "\n",
    "with open(\"output.csv\", mode = \"w\") as f:\n",
    "    f.write(str(count))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(927, 685)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "reuters_stopwords=set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "df = pd.read_csv('Reuters.csv')\n",
    "df['article_text'] = df['article_text'].map(lambda x: x.encode('unicode-escape').decode('utf-8'))\n",
    "df = df.drop(df.index[[0]])\n",
    "df_crud = df['target'] == 'crude'\n",
    "dfcrude = df[df_crud]\n",
    "df_mon = df['target'] == 'money'\n",
    "dfmoney = df[df_mon]\n",
    "df_tokmon = (dfmoney['article_text'].apply(word_tokenize))\n",
    "dfm = df_tokmon.tolist()\n",
    "dfm_flat = []\n",
    "for sublist in dfm:\n",
    "    for item in sublist:\n",
    "        dfm_flat.append(item)\n",
    "uniq_money = len(set(dfm_flat))\n",
    "\n",
    "df_tokcrude = (dfcrude['article_text'].apply(word_tokenize))\n",
    "dfd = df_tokcrude.tolist()\n",
    "dfd_flat = []\n",
    "for sublist in dfd:\n",
    "    for item in sublist:\n",
    "        dfd_flat.append(item)\n",
    "uniq_crude = len(set(dfd_flat))\n",
    "\n",
    "df_tok = df['article_text'].apply(word_tokenize)\n",
    "dft = df_tok.tolist()\n",
    "dft_flat = []\n",
    "for sublist in dft:\n",
    "    for item in sublist:\n",
    "        dft_flat.append(item)\n",
    "\n",
    "filter = []\n",
    "for w in dft_flat:\n",
    "    if w.lower() not in reuters_stopwords:\n",
    "        filter.append(w)\n",
    "wordnet_lemm = WordNetLemmatizer()\n",
    "voca = []\n",
    "for word in filter:\n",
    "    voc = wordnet_lemm.lemmatize(word)\n",
    "    voca.append(voc)\n",
    "len(voca)\n",
    "vectorizer = TfidfVectorizer(min_df=25,max_df=500,lowercase=True,stop_words=reuters_stopwords)\n",
    "vectors = vectorizer.fit_transform(df['article_text'])\n",
    "tfidf_size = vectors.shape\n",
    "\n",
    "with open(\"output.csv\", mode = \"w\") as f:\n",
    "    f.write(str(uniq_money))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(count2))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(count3))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "#df = pd.read_csv(\"https://.zip\")\n",
    "df = pd.read_csv(\"suicides.csv\")\n",
    "df1 = df[['year', 'suicides/100k pop']]\n",
    "df2 = df1.groupby('year', as_index = False).sum()\n",
    "\n",
    "df2_1 = df2.query(\"year==1985\")\n",
    "df2_2 = df2.query(\"year==2015\")\n",
    "sp_1985 = df2_1[\"suicides/100k pop\"].iloc[0]\n",
    "sp_2015 = df2_2[\"suicides/100k pop\"].iloc[0]\n",
    "\n",
    "pct_change = float(((sp_1985 - sp_2015) * 100) / sp_2015)\n",
    "pct_change = round(pct_change, 3)\n",
    "\n",
    "with open(\"output1.csv\", mode = \"w\") as f:\n",
    "    f.write(str(pct_change))\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#df = pd.read_csv(\"https://.zip\")\n",
    "df = pd.read_csv(\"CricketMatchDataset.csv\")\n",
    "mean = df.agg({\"Team 1 20ovr Score\" : np.mean}).values\n",
    "mean = round(*mean, 3)\n",
    "\n",
    "count = df[\"Final Result\"].str.contains(r'Home Team').sum()\n",
    "\n",
    "df1 = df[['Home Team', 'Team 1 Batsman 1 Score']]\n",
    "df2 = df1.groupby('Home Team', as_index = False).mean()\n",
    "df_i = df2[df2['Home Team'] == 'India']\n",
    "avg_i = round(df_i[\"Team 1 Batsman 1 Score\"].iloc[0], 3)\n",
    "\n",
    "df1 = df[((df['Home Team'] == 'India') & (df['Team Batting First'] == 'Home Team'))]\n",
    "df2 = df[((df['Away Team'] == 'India') & (df['Team Batting First'] == 'Away Team'))]\n",
    "df3 = df1.append(df2)\n",
    "mean_i = round(df3[\"Team 1 Batsman 1 Score\"].mean(axis = 0), 3)\n",
    "\n",
    "df.drop(['Date of Match'], axis = 1, inplace = True)\n",
    "X = df.drop('Final Result', axis = 1)\n",
    "X = pd.get_dummies(X, prefix_sep = '_', drop_first = False)\n",
    "no_of_col = len(X.columns)\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(df['Final Result'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "lr_cric = LogisticRegression(C = 1.0, solver = 'liblinear', multi_class = 'ovr')\n",
    "lr_cric.fit(X_train, y_train)\n",
    "y_pred = lr_cric.predict(X_test)\n",
    "acc = round(accuracy_score(y_test, y_pred), 3)\n",
    "\n",
    "with open(\"output.csv\", mode = \"w\") as f:\n",
    "    f.write(str(mean))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(count))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(avg_i))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(mean_i))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(no_of_col))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(acc))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"BankLoan.csv\")\n",
    "\n",
    "dfm = pd.DataFrame(df.isnull().sum())\n",
    "dfm.rename(columns = {0:'Missing_Values'}, inplace = True)\n",
    "dfm.index.names = ['Column']\n",
    "dfm.drop(dfm[dfm['Missing_Values'] == 0].index, inplace = True)\n",
    "dfm = dfm.sort_values(by = 'Missing_Values')\n",
    "dfm.insert(1, \"Impute_Method\", [\"Mode\", \"Mode\", \"Mean\", \"Mode\", \"Mean\", \"Mode\", \"Mode\"], True)\n",
    "dfm.to_csv('Impute.csv')\n",
    "\n",
    "rows = df.shape[0]\n",
    "dups = df.pivot_table(index = ['Gender', 'Married', 'Education', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status'], aggfunc ='size')\n",
    "\n",
    "dfc = df.corr().unstack().sort_values(ascending = False).drop_duplicates()\n",
    "hcv = round(dfc[1], 3)\n",
    "\n",
    "df.dropna(inplace = True)\n",
    "X = df.drop('Loan_Status', axis = 1)\n",
    "y = df['Loan_Status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "r, c = X_test.shape\n",
    "\n",
    "with open(\"output.csv\", mode = \"w\") as f:\n",
    "    f.write(str(rows))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(len(dups)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(hcv))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(str(r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
